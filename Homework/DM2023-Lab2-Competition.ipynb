{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct BERT model to do classification\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(1)\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_json_file(file_path):\n",
    "    total_line = 0\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            total_line += 1\n",
    "            json_line = json.loads(line)\n",
    "            data.append(json_line)\n",
    "    return data, total_line\n",
    "\n",
    "file_path = 'tweets_DM.json'\n",
    "tweets_data, lines = read_json_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((tweets_data[0][\"_source\"][\"tweet\"]))\n",
    "clean_dicts = []\n",
    "for i in range(lines):\n",
    "    clean_dicts.append({\"ids\":tweets_data[i][\"_source\"][\"tweet\"][\"tweet_id\"], \"text\":tweets_data[i][\"_source\"][\"tweet\"][\"text\"], \"Type\":\"train\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read label from emotion.csv\n",
    "emotion_df = pd.read_csv('emotion.csv')\n",
    "data_type = pd.read_csv('data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_set = set(data_type[data_type['identification'] == 'train']['tweet_id'])\n",
    "test_id_set = data_type[data_type['identification'] == 'test']['tweet_id']\n",
    "emotion_dict = dict(zip(emotion_df['tweet_id'], emotion_df['emotion']))\n",
    "for dic in tqdm(clean_dicts):\n",
    "    if dic['ids'] in train_id_set:\n",
    "        dic['label'] = emotion_dict[dic['ids']]\n",
    "    else:\n",
    "        dic['Type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract type == train from X\n",
    "train_dicts = []\n",
    "test_dicts = []\n",
    "for idx, dic in enumerate(clean_dicts):\n",
    "    if dic['Type'] == 'train':\n",
    "        train_dicts.append(dic)\n",
    "    else:\n",
    "        test_dicts.append(dic)\n",
    "print(len(train_dicts))\n",
    "print(len(test_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do statistics on train_dicts\n",
    "\n",
    "emotion_counter = Counter()\n",
    "for dic in train_dicts:\n",
    "    emotion_counter[dic['label']] += 1\n",
    "print(emotion_counter)\n",
    "# extract each emotion from train_dicts with the same number of least emotion\n",
    "emotion_num = emotion_counter.most_common()[-1][1]\n",
    "print(emotion_num)\n",
    "# emotion_num = 1000\n",
    "train_dicts_same = []\n",
    "for key in emotion_counter.keys():\n",
    "    target_dict = [dic for dic in train_dicts if dic['label'] == key]\n",
    "    random.shuffle(target_dict)\n",
    "    train_dicts_same += target_dict[:]\n",
    "print(len(train_dicts_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do one-hot encoding on train_dicts_same\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "X = [dic['text'] for dic in train_dicts_same]\n",
    "y = [dic['label'] for dic in train_dicts_same]\n",
    "label_y = label_encoder.fit_transform(y)\n",
    "encode_y = one_hot_encoder.fit_transform(label_y.reshape(-1, 1)).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, encode_y, test_size=0.1, random_state=1)\n",
    "val_X, test_X, val_y, test_y = train_test_split(val_X, val_y, test_size=0.5, random_state=1)\n",
    "# do statistics on train_y\n",
    "\n",
    "train_y = np.argmax(train_y, axis=1)\n",
    "val_y = np.argmax(val_y, axis=1)\n",
    "test_y = np.argmax(test_y, axis=1)\n",
    "print(Counter(train_y))\n",
    "print(Counter(val_y))\n",
    "print(Counter(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model class for BERT\n",
    "\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 8\n",
    "\n",
    "        # instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, D_out),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # extract last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertClassifier(freeze_bert=False)\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and learning rate scheduler\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "# number of training epochs\n",
    "epochs = 1\n",
    "# number of batches\n",
    "batch_size = 32\n",
    "# calculate number of training steps\n",
    "num_train_steps = int(len(train_X) / batch_size * epochs)\n",
    "# create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_train_steps)\n",
    "# define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train model\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    # start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val F1 ':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\" * 70)\n",
    "        # measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        # reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        # put the model into the training mode\n",
    "        model.train()\n",
    "        # for each batch of training data\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch_counts += 1\n",
    "            # load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            # zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            # perform forward pass\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            # compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            # perform backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                # print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                # reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        # calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"-\" * 70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # after the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy, val_f1_score = evaluate(model, val_dataloader)\n",
    "            # print validation results\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_f1_score:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 70)\n",
    "        print(\"\\n\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "# define function for evaluation\n",
    "def evaluate(model, val_dataloader):\n",
    "    # put the model into the evaluation mode\n",
    "    model.eval()\n",
    "    # tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    # for each batch in our validation set\n",
    "    val_f1_score = []\n",
    "    for batch in val_dataloader:\n",
    "        # load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        # compute loss and accuracy\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        # get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        # calculate the accuracy rate\n",
    "        # calculate the f1 score\n",
    "        f1_score_macro = f1_score(b_labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_f1_score.append(f1_score_macro)\n",
    "        val_accuracy.append(accuracy)\n",
    "    # compute the average accuracy and loss over the validation set\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    val_f1_score = np.mean(val_f1_score)\n",
    "    return val_loss, val_accuracy, val_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train data to torch tensor\n",
    "train_inputs = tokenizer(train_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "train_labels = torch.tensor(train_y)\n",
    "# convert validation data to torch tensor\n",
    "val_inputs = tokenizer(val_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "val_labels = torch.tensor(val_y)\n",
    "# convert test data to torch tensor\n",
    "test_inputs = tokenizer(test_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "test_labels = torch.tensor(test_y)\n",
    "\n",
    "# create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "# create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=epochs, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy and f1 score on the test set\n",
    "test_loss, test_accuracy, test_f1_score = evaluate(model, test_dataloader)\n",
    "# print the accuracy and loss on the test set\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), 'BERT_model_v3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = BertClassifier(freeze_bert=False)\n",
    "model.load_state_dict(torch.load('BERT_model_v3.bin'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do predict on test_dicts, which has no label\n",
    "test_inputs = tokenizer([dic['text'] for dic in test_dicts], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# put the model into the evaluation mode\n",
    "model.eval()\n",
    "# tracking variables\n",
    "predictions = []\n",
    "# predict\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # load batch to GPU\n",
    "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "        # model predictions\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "    # get the predictions\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    # put the predicted labels to a list\n",
    "    predictions += preds.cpu().numpy().tolist()\n",
    "# get the prediction result\n",
    "predictions = label_encoder.inverse_transform(predictions)\n",
    "# print the result into a csv file\n",
    "output_df = pd.DataFrame({'id': [dic['ids'] for dic in test_dicts], 'emotion': predictions})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(output_df, 'submission_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
