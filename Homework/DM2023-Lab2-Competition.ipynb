{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在這次的competition中，我主要使用Roberta為架構，以訓練對文字八分類的model。一開始我有針對原始資料做在Lab1與Lab2中練習的feature extraction，像是移除stopwords以及標點符號，但是在這次的competition中，我發現這些feature extraction的方法對於model的表現並沒有太大的幫助(在kaggle上的public score約0.3)，反而是會讓model的表現下降，因此我最後選擇使用原始的資料進行訓練。原先是使用bert-base-uncased，但在我的實驗後發現roberta-base的表現最好。\n",
    "#### 這次的資料集有非常嚴重的unbalanced問題，因此一開始我down sample了資料，讓所有資料的數量與最少數量的label(anger)資料數量相同。但是在實驗後發現，我自己test data的f1 score(~0.48)會比kaggle的public score(~0.42)高，因此我推斷kaggle上的test data也有unbalanced的問題，因此最後我使用全部的資料，在最後的訓練中，以其中99%作為進行訓練，0.5%為validation，0.5%為test。以下是我的各種模型在kaggle上的public score與private score。\n",
    "\n",
    "| Model | public score | private score |\n",
    "|:-----|-----|-----:|\n",
    "|BERT (down sample)|0.42282|0.4066| \n",
    "|RoBERTa (down sample)|0.53448|0.51749|\n",
    "|RoBERTa (all data)|**0.54686**|**0.52984**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/andrewchen/andrewchen/DM/DMLab2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Construct BERT model to do classification\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "random.seed(1)\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1867535it [00:33, 56367.52it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_json_file(file_path):\n",
    "    total_line = 0\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            total_line += 1\n",
    "            json_line = json.loads(line)\n",
    "            data.append(json_line)\n",
    "    return data, total_line\n",
    "\n",
    "file_path = '../../tweets_DM.json'\n",
    "tweets_data, lines = read_json_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}\n"
     ]
    }
   ],
   "source": [
    "# Read data from json file\n",
    "print((tweets_data[0][\"_source\"][\"tweet\"]))\n",
    "clean_dicts = []\n",
    "for i in range(lines):\n",
    "    clean_dicts.append({\"ids\":tweets_data[i][\"_source\"][\"tweet\"][\"tweet_id\"], \"text\":tweets_data[i][\"_source\"][\"tweet\"][\"text\"], \"Type\":\"train\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read label from emotion.csv\n",
    "emotion_df = pd.read_csv('../emotion.csv')\n",
    "data_type = pd.read_csv('../data_identification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1867535/1867535 [00:02<00:00, 631198.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# determine the type(train or test) of each tweet\n",
    "train_id_set = set(data_type[data_type['identification'] == 'train']['tweet_id'])\n",
    "test_id_set = data_type[data_type['identification'] == 'test']['tweet_id']\n",
    "emotion_dict = dict(zip(emotion_df['tweet_id'], emotion_df['emotion']))\n",
    "for dic in tqdm(clean_dicts):\n",
    "    if dic['ids'] in train_id_set:\n",
    "        dic['label'] = emotion_dict[dic['ids']]\n",
    "    else:\n",
    "        dic['Type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455563\n",
      "411972\n"
     ]
    }
   ],
   "source": [
    "# extract type == train from X\n",
    "train_dicts = []\n",
    "test_dicts = []\n",
    "for idx, dic in enumerate(clean_dicts):\n",
    "    if dic['Type'] == 'train':\n",
    "        train_dicts.append(dic)\n",
    "    else:\n",
    "        test_dicts.append(dic)\n",
    "print(len(train_dicts))\n",
    "print(len(test_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'joy': 516017, 'anticipation': 248935, 'trust': 205478, 'sadness': 193437, 'disgust': 139101, 'fear': 63999, 'surprise': 48729, 'anger': 39867})\n",
      "39867\n",
      "1455563\n"
     ]
    }
   ],
   "source": [
    "# do statistics on train_dicts\n",
    "\n",
    "emotion_counter = Counter()\n",
    "for dic in train_dicts:\n",
    "    emotion_counter[dic['label']] += 1\n",
    "print(emotion_counter)\n",
    "# extract each emotion from train_dicts with the same number of least emotion\n",
    "emotion_num = emotion_counter.most_common()[-1][1]\n",
    "print(emotion_num)\n",
    "train_dicts_same = []\n",
    "for key in emotion_counter.keys():\n",
    "    target_dict = [dic for dic in train_dicts if dic['label'] == key]\n",
    "    random.shuffle(target_dict)\n",
    "    train_dicts_same += target_dict[:]\n",
    "    # train_dicts_same += target_dict[:emotion_num]\n",
    "print(len(train_dicts_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do one-hot encoding on train_dicts_same\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "X = [dic['text'] for dic in train_dicts_same]\n",
    "y = [dic['label'] for dic in train_dicts_same]\n",
    "label_y = label_encoder.fit_transform(y)\n",
    "encode_y = one_hot_encoder.fit_transform(label_y.reshape(-1, 1)).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 510762, 1: 246450, 7: 203453, 5: 191447, 2: 137779, 3: 63362, 6: 48270, 0: 39484})\n",
      "Counter({4: 2584, 1: 1287, 5: 1010, 7: 1008, 2: 638, 3: 336, 6: 231, 0: 184})\n",
      "Counter({4: 2671, 1: 1198, 7: 1017, 5: 980, 2: 684, 3: 301, 6: 228, 0: 199})\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, encode_y, test_size=0.01, random_state=1)\n",
    "val_X, test_X, val_y, test_y = train_test_split(val_X, val_y, test_size=0.5, random_state=1)\n",
    "# do statistics on train_y\n",
    "\n",
    "train_y = np.argmax(train_y, axis=1)\n",
    "val_y = np.argmax(val_y, axis=1)\n",
    "test_y = np.argmax(test_y, axis=1)\n",
    "print(Counter(train_y))\n",
    "print(Counter(val_y))\n",
    "print(Counter(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model class for BERT\n",
    "\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 8\n",
    "\n",
    "        # instantiate BERT model\n",
    "        self.bert = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, D_out),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # extract last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cluster/home/andrewchen/andrewchen/DM/DMLab2/.venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(freeze_bert=False)\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and learning rate scheduler\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "# number of training epochs\n",
    "epochs = 1\n",
    "# number of batches\n",
    "batch_size = 32\n",
    "# calculate number of training steps\n",
    "num_train_steps = int(len(train_X) / batch_size * epochs)\n",
    "# create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=num_train_steps)\n",
    "# define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train model\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    # start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val F1 ':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\" * 70)\n",
    "        # measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        # reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        # put the model into the training mode\n",
    "        model.train()\n",
    "        # for each batch of training data\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch_counts += 1\n",
    "            # load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            # zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            # perform forward pass\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            # compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            # perform backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "            # clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 1000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                # print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                # reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        # calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"-\" * 70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # after the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy, val_f1_score = evaluate(model, val_dataloader)\n",
    "            # print validation results\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {val_f1_score:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 70)\n",
    "        print(\"\\n\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "# define function for evaluation\n",
    "def evaluate(model, val_dataloader):\n",
    "    # put the model into the evaluation mode\n",
    "    model.eval()\n",
    "    # tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    # for each batch in our validation set\n",
    "    val_f1_score = []\n",
    "    for batch in val_dataloader:\n",
    "        # load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        # compute loss and accuracy\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "        # get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        # calculate the accuracy rate\n",
    "        # calculate the f1 score\n",
    "        f1_score_macro = f1_score(b_labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_f1_score.append(f1_score_macro)\n",
    "        val_accuracy.append(accuracy)\n",
    "    # compute the average accuracy and loss over the validation set\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    val_f1_score = np.mean(val_f1_score)\n",
    "    return val_loss, val_accuracy, val_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train data to torch tensor\n",
    "train_inputs = tokenizer(train_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "train_labels = torch.tensor(train_y)\n",
    "# convert validation data to torch tensor\n",
    "val_inputs = tokenizer(val_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "val_labels = torch.tensor(val_y)\n",
    "# convert test data to torch tensor\n",
    "test_inputs = tokenizer(test_X, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "test_labels = torch.tensor(test_y)\n",
    "\n",
    "# create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "# create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Val F1   |  Elapsed \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1001/45032 [05:22<4:02:35,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  1000   |   1.926023   |     -      |     -     |     -     |  322.26  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2001/45032 [10:28<3:35:17,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  2000   |   1.877168   |     -      |     -     |     -     |  305.89  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3001/45032 [15:31<3:31:15,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  3000   |   1.852217   |     -      |     -     |     -     |  303.85  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4001/45032 [20:35<3:29:05,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  4000   |   1.793487   |     -      |     -     |     -     |  303.85  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5001/45032 [25:39<3:23:56,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  5000   |   1.784153   |     -      |     -     |     -     |  303.63  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6001/45032 [30:43<3:16:14,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  6000   |   1.771376   |     -      |     -     |     -     |  304.17  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 7001/45032 [35:46<3:14:13,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  7000   |   1.764161   |     -      |     -     |     -     |  303.24  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 8001/45032 [40:50<3:06:53,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  8000   |   1.763137   |     -      |     -     |     -     |  303.76  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 9001/45032 [45:53<3:04:06,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  9000   |   1.757923   |     -      |     -     |     -     |  303.33  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 10001/45032 [50:58<2:57:06,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  10000  |   1.758411   |     -      |     -     |     -     |  304.22  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11001/45032 [56:02<2:53:48,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  11000  |   1.756003   |     -      |     -     |     -     |  303.97  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 12001/45032 [1:01:06<2:47:30,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  12000  |   1.753402   |     -      |     -     |     -     |  304.10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 13001/45032 [1:06:09<2:42:33,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  13000  |   1.748551   |     -      |     -     |     -     |  303.62  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 14001/45032 [1:11:13<2:35:38,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  14000  |   1.742421   |     -      |     -     |     -     |  303.92  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 15001/45032 [1:16:17<2:31:13,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  15000  |   1.744478   |     -      |     -     |     -     |  303.76  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 16001/45032 [1:21:21<2:28:33,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  16000  |   1.744048   |     -      |     -     |     -     |  304.11  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 17001/45032 [1:26:25<2:22:31,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  17000  |   1.737032   |     -      |     -     |     -     |  304.33  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 18001/45032 [1:31:29<2:16:43,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  18000  |   1.732790   |     -      |     -     |     -     |  303.73  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 19001/45032 [1:36:33<2:12:51,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  19000  |   1.725424   |     -      |     -     |     -     |  304.21  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 20001/45032 [1:41:37<2:07:34,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  20000  |   1.714170   |     -      |     -     |     -     |  303.49  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 21001/45032 [1:46:41<2:00:49,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  21000  |   1.710770   |     -      |     -     |     -     |  303.97  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 22001/45032 [1:51:45<1:56:09,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  22000  |   1.705896   |     -      |     -     |     -     |  303.70  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 23001/45032 [1:56:49<1:51:47,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  23000  |   1.702272   |     -      |     -     |     -     |  304.17  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 24001/45032 [2:01:53<1:47:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  24000  |   1.697825   |     -      |     -     |     -     |  303.93  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 25001/45032 [2:06:57<1:41:56,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  25000  |   1.694125   |     -      |     -     |     -     |  304.29  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 26001/45032 [2:12:01<1:38:00,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  26000  |   1.696137   |     -      |     -     |     -     |  303.89  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 27001/45032 [2:17:05<1:31:38,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  27000  |   1.689062   |     -      |     -     |     -     |  304.12  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 28001/45032 [2:22:09<1:25:59,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  28000  |   1.685796   |     -      |     -     |     -     |  303.67  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 29001/45032 [2:27:12<1:22:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  29000  |   1.691085   |     -      |     -     |     -     |  303.36  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 30001/45032 [2:32:15<1:16:13,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  30000  |   1.686264   |     -      |     -     |     -     |  303.12  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 31001/45032 [2:37:18<1:11:18,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  31000  |   1.684070   |     -      |     -     |     -     |  303.16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 32001/45032 [2:42:22<1:05:57,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  32000  |   1.683492   |     -      |     -     |     -     |  303.37  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 33001/45032 [2:47:25<1:01:13,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  33000  |   1.687469   |     -      |     -     |     -     |  303.82  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 34001/45032 [2:52:29<54:57,  3.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  34000  |   1.683497   |     -      |     -     |     -     |  303.29  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 35001/45032 [2:57:32<50:47,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  35000  |   1.683292   |     -      |     -     |     -     |  302.90  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 36001/45032 [3:02:35<45:55,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  36000  |   1.678795   |     -      |     -     |     -     |  303.29  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 37001/45032 [3:07:39<40:36,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  37000  |   1.680640   |     -      |     -     |     -     |  303.56  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 38001/45032 [3:12:42<35:15,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  38000  |   1.673865   |     -      |     -     |     -     |  303.59  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 39001/45032 [3:17:46<30:38,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  39000  |   1.675897   |     -      |     -     |     -     |  304.08  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 40001/45032 [3:22:50<25:22,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  40000  |   1.674445   |     -      |     -     |     -     |  303.55  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 41001/45032 [3:27:54<20:33,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  41000  |   1.671047   |     -      |     -     |     -     |  303.91  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 42001/45032 [3:32:57<15:23,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  42000  |   1.670482   |     -      |     -     |     -     |  303.26  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 43001/45032 [3:38:00<10:20,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  43000  |   1.671384   |     -      |     -     |     -     |  303.51  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 44001/45032 [3:43:05<05:13,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  44000  |   1.670280   |     -      |     -     |     -     |  304.28  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 45001/45032 [3:48:08<00:09,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  45000  |   1.673788   |     -      |     -     |     -     |  303.73  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45032/45032 [3:48:18<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  45031  |   1.685205   |     -      |     -     |     -     |   9.51   \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |    -    |   1.723712   |  1.658355  |   61.34   |   0.45    | 13712.42 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, epochs=epochs, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 61.39176065162907\n",
      "Test F1 Score: 0.4463059730763\n"
     ]
    }
   ],
   "source": [
    "# compute the accuracy and f1 score on the test set\n",
    "test_loss, test_accuracy, test_f1_score = evaluate(model, test_dataloader)\n",
    "# print the accuracy and loss on the test set\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), '../../BERT_model_v3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=8, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = BertClassifier(freeze_bert=False)\n",
    "model.load_state_dict(torch.load('../../BERT_model_v3.bin'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do predict on test_dicts, which has no label\n",
    "test_inputs = tokenizer([dic['text'] for dic in test_dicts], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12875/12875 [19:25<00:00, 11.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# put the model into the evaluation mode\n",
    "model.eval()\n",
    "# tracking variables\n",
    "predictions = []\n",
    "# predict\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # load batch to GPU\n",
    "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "        # model predictions\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "    # get the predictions\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    # put the predicted labels to a list\n",
    "    predictions += preds.cpu().numpy().tolist()\n",
    "# get the prediction result\n",
    "predictions = label_encoder.inverse_transform(predictions)\n",
    "# print the result into a csv file\n",
    "output_df = pd.DataFrame({'id': [dic['ids'] for dic in test_dicts], 'emotion': predictions})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(output_df, 'submission_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由於資料量真的很大，tokenize data就要十分鐘左右，訓練一個epoch就要花上三個半小時，訓練好之後讓model reference到要交到kaggle上的test data也要20分鐘。從loss的下降來看，訓練很多epoch並沒有甚麼幫助(也浪費時間跟計算資源)，因此我分數最高的版本也只有訓練兩個epoch而已(有load checkpoint然後再訓練一次，因此notebook中的epoch設為1)。\n",
    "#### 在這次competition中，比較令我意外的是feature extraction對RoBERTa以及BERT model並沒有幫助，反而讓模型的效果下降不少，我認為原因是這些feature extraction的方式影響到了原本的語意，因此不適合用在BERT-base model上再做embedding，反而是這樣的pretrained model可以自己學習到語意以及這些資訊，因此不適合、不需要額外再做之前Lab中練習的feature extraction。但這不代表feature extraction沒有意義，但這裡我並沒有找到針對BERT-base model適合的feature extraction方式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
